# 김도겸

생성일: April 3, 2023 3:24 PM

![Untitled](%E1%84%80%E1%85%B5%E1%86%B7%E1%84%83%E1%85%A9%E1%84%80%E1%85%A7%E1%86%B7%2016996f18478f40c8b06faa67a6e323ac/Untitled.png)

BERT는 Bidirectional Encoder Representations from Transformer이다.

통일된 구조의 Model Architecture를 만들고, 다중 레이어 양방향 트랜스포머 인코더를 사용한다.

Pre-training: MLM, NSP

Fine-tuning: Downstreaming task

MLM: Masked Language model

→ 마스킹 과정을 거쳐서 토큰을 전달

![Untitled](%E1%84%80%E1%85%B5%E1%86%B7%E1%84%83%E1%85%A9%E1%84%80%E1%85%A7%E1%86%B7%2016996f18478f40c8b06faa67a6e323ac/Untitled%201.png)

Input Sequence에서 15%의 WordPiece token을 masking함.

15% token을 전부 masking하는 것이 아니라 80%는 MASK로 바꾸고, 나머지는 다른 token으로, 나머지 10%는 본래 token으로 둠.

NSP: Next Sentence Prediction